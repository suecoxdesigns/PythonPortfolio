# -*- coding: utf-8 -*-
"""
Created on Mon Jan 16 15:34:23 2023

@author: sueco
"""

#%%  Naive Bayes classifiers  

########################################

#related to linear regression
# based on simple probabilistic models of how the data in each class might have been generated

# assume each feature of an instance is independent of all the others

# highly efficient learning and prediction
#generalization performance may be worse than more sophisticated learning methods

# thee flavors

# Bernoulii: binary features
# Multinomial: discrete features (word counts)
# Gaussian: continous/real-valued features
    # statistics computer for ecah class Mean, SD
    # assumes the data for each class generated by a simple gauussian distribution
    # Predicting the class of a new data point corresponds mathematically to estimating the probability that each classes gaussian distribution was most likely to have generated the data point. Classifier pickes the class with highest probability
    # decision boundary usually parabolic - uness variance the sme between classes - and htne linear
    
    
    ################
    
    # typically used for high-dimensional data
    # when each data instance has throusands or may even more features.
    
    ########################
    
    
# Pros:
    # Easy to understand
    # simple, efficient parameter estimation
    # works well with high dimensional data
    # often useful as a baseline comparison against more sophisiticated methods
    
    
# Cons
    # Assumption that features are conditionally independent
    # As a result, other classifiers types often have better gneralization perfromance
    # Their confidence estimates for predictions are note very accurate


import matplotlib,pytorch as plt 
from sklearn.naive_bayes import GaussianNB
from adspy_shared_utilities import plot_class_regions_for_classifier
from sklearn.datasets import make_classification, make_blobs
from matplotlib.colors import ListedColormap
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_breast_cancer


cmap_bold = ListedColormap(['#FFFF00', '#00FF00', '#0000FF','#000000'])

# synthetic dataset for classification (binary)
plt.figure()
plt.title('Sample binary classification problem with two informative features')
X_C2, y_C2 = make_classification(n_samples = 100, n_features=2,
                                n_redundant=0, n_informative=2,
                                n_clusters_per_class=1, flip_y = 0.1,
                                class_sep = 0.5, random_state=0)
plt.scatter(X_C2[:, 0], X_C2[:, 1], marker= 'o', c=y_C2, s=50, cmap=cmap_bold)
plt.show()

X_train, X_test, y_train, y_test, = train_test_split(X_C2, y_C2, random_state=0)

nbclf = GaussianNB().fit(X_train, y_train)

# can use a 'partial fit' rather than fit if you're using a hugh data set

plot_class_regions_for_classifier(nbclf, X_train, y_train, X_test, y_test,'Gaussian Naive Bayes classifier: Dataset 1')


#%%

# more difficult synthetic dataset for classification (binary)
# with classes that are not linearly separable
X_D2, y_D2 = make_blobs(n_samples = 100, n_features = 2,
                       centers = 8, cluster_std = 1.3,
                       random_state = 4)
y_D2 = y_D2 % 2
plt.figure()
plt.title('Sample binary classification problem with non-linearly separable classes')
plt.scatter(X_D2[:,0], X_D2[:,1], c=y_D2,
           marker= 'o', s=50, cmap=cmap_bold)
plt.show()
X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2,
                                                   random_state=0)

nbclf = GaussianNB().fit(X_train, y_train)
plot_class_regions_for_classifier(nbclf, X_train, y_train, X_test, y_test,
                                 'Gaussian Naive Bayes classifier: Dataset 2')

#%% Application to a real-world dataset

# Breast cancer dataset for classification
cancer = load_breast_cancer()
(X_cancer, y_cancer) = load_breast_cancer(return_X_y = True)

X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state = 0)

nbclf = GaussianNB().fit(X_train, y_train)
print('Breast cancer dataset')
print('Accuracy of GaussianNB classifier on training set: {:.2f}'
     .format(nbclf.score(X_train, y_train)))
print('Accuracy of GaussianNB classifier on test set: {:.2f}'
     .format(nbclf.score(X_test, y_test)))


